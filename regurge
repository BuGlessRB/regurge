#!/usr/bin/nodejs

    /** @license
    ** regurge: Node.js script for chat with Google Vertex AI Gemini API,
   **                  providing an interactive and JSON-based interface.
  ** Copyright (c) 2025 by Stephen R. van den Berg <srb@cuci.nl>
 ** License: ISC OR GPL-3.0
** Sponsored by: Cubic Circle, The Netherlands
*/

// Implicitly uses the GOOGLE_APPLICATION_CREDENTIALS environment variable
// to authenticate to the API.  There are fallbacks in place which drop
// back down to "gcloud auth application-default login".

//var modelname = "gemini-flash-lite-latest";
var modelname = "gemini-2.5-flash-lite";

var project = process.env.GOOGLE_CLOUD_PROJECT
           || process.env.VERTEXAI_PROJECT;
var location = process.env.GOOGLE_CLOUD_LOCATION
            || process.env.VERTEXAI_LOCATION;

const readline = require("readline");
const {
  HarmBlockMethod,
  HarmBlockThreshold,
  HarmCategory,
  Modality,
  GoogleGenAI,
} = require('@google/genai');

  // This has been copied from regurge.vim.
 //  regurge only uses this in interactive mode.
//   In JSON mode the systemInstruction is passed as data.
const cmdline_def_systeminstruction = [
  { text: `Be exceedingly brief, succinct, blunt, direct.
Articulate doubt if unsure.
Answer in staccato keywords by default.
When prompted to review input: add a summary of all issues at the top,
use unified-diff for suggested changes,
suppress whitespace or comment changes unless syntactically relevant.
You are addressing a senior developer/physicist.
Respond in the prompt-language by default.
No preamble, politeness, compliments, apologies, disclaimers.
Your name is 'Regurge'.`
  }
];

var abortctrl;

const masterconfig = {
  model: modelname,
  config: {
    // To curtail unforeseen cost-explosion.
    //maxOutputTokens: 131072,

    // responseMimeType: "application/json",
    responseModalities: [Modality.TEXT,
                        // Modality.IMAGE, Modality.AUDIO,
                        ],

    // The following settings make the model's responses deterministic.
    temperature: 0.0,
    topP: 0.0001,
    topK: 1.0,
    seed: 42,

    // Style points, keep it neutral: do not enforce.
    frequencyPenalty: 0.4, // -2.0 .. 2.0
    presencePenalty: 0.4,  // -2.0 .. 2.0
    enableAffectiveDialog: false,

     thinkingConfig: {
      includeThoughts: false,
      thinkingBudget: 0,
    },
    candidateCount: 1,
    // Probabilities
    // responseLogprobs: true,
    // logprobs: 4,  // Show the four runners up alternative probs

    // modelSelectionConfig:
    //   FeatureSelectionPreference.FEATURE_SELECTION_PREFERENCE_UNSPECIFIED,
    //   FeatureSelectionPreference.PRIORITIZE_QUALITY,
    //   FeatureSelectionPreference.BALANCED,
    //   FeatureSelectionPreference.PRIORITIZE_COST,

    // routingConfig: {
    //   manualMode: {
    //     modelName: modelname,
    //   }
      // autoMode: {
      //   modelRoutingPreference: "UNKNOWN"
      //   modelRoutingPreference: "PRIORITIZE_QUALITY"
      //   modelRoutingPreference: "BALANCED"
      //   modelRoutingPreference: "PRIORITIZE_COST"
      // }
    // },
    tools: [
      { googleSearch: { excludeDomains: [] } },
      //{ googleMaps: { enableWidget: true } },
      //{ enterpriseWebSearch: { excludeDomains: [] } },
      //{ codeExecution: {} },
      //{ urlContext: {} },
    ],
    toolConfig: {
      retrievalConfig: {
        languageCode: "en"
      }
    },
    safetySettings: [
      // Uncensored
      { category: HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_HARASSMENT,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_IMAGE_DANGEROUS_CONTENT,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_IMAGE_HARASSMENT,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_IMAGE_HATE,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_IMAGE_SEXUALLY_EXPLICIT,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold: HarmBlockThreshold.OFF, },
      { category: HarmCategory.HARM_CATEGORY_UNSPECIFIED,
        threshold: HarmBlockThreshold.OFF, },
    ],
  },
};

function showusage(msg) {
  console.error(msg);
  process.exit(64);
}

for (let i = 0; i < process.argv.length; i++) {
  if (i + 1 < process.argv.length) {
    if (process.argv[i] === "-P")
      project = process.argv[++i];
    else if (process.argv[i] === "-L")
      location = process.argv[++i];
    else if (process.argv[i] === "-M")
      modelname = process.argv[++i];
  }
}

if (!project || !location) {
  showusage(
    "Error: Please set VERTEXAI_PROJECT and VERTEXAI_LOCATION " +
      "environment variables or provide them as flags."
  );
}

const gen_ai = new GoogleGenAI({
  vertexai: true,
  project: project,
  location: location,
});

const isVerboseMode = process.argv.includes("-v");
const isJsonMode = process.argv.includes("-j");

/**
 * Calls the Vertex AI Gemini model with the provided conversation contents.
 * The model implicitly handles caching of common prefixes in the conversation.
 * This function does NOT manage chat history; it only performs the API call.
 * @param {Array<Object>} contents The conversation history or messages to send
 *                                 to the model. Each object should have
 *                                 "role" and "parts".
 *                                 (e.g., { "role": "user",
 *                                          "parts": [{ "text": "..." }] })
 * @returns {Promise<Array<Object>>} The full text response from the model.
 */
async function* sendtoLLM(systemconfig, contents) {
   // Create a new control on every response, otherwise we get warnings
  //  about too many (not-yet-garbage collected) listeners
  abortctrl = new AbortController();
  if (!contents.length) {
    console.error(
      "Error: Contents for Gemini API must be a non-empty array."
    );
      // In JSON mode, this would be a fatal error. In interactive, it might
     //  just skip.
    //   For now, let's make it fatal for consistency.
    process.exit(1);
  }

  const modelconfig = Object.assign({}, masterconfig);
  Object.assign(modelconfig.config, systemconfig);
  modelconfig.contents = contents;
  modelconfig.config.abortSignal = abortctrl.signal;

  try {
    let promptbytecount = 0;
    for (const part of modelconfig.config.systemInstruction)
      promptbytecount += part.text.length;
    for (const turn of contents) {
      for (const part of turn.parts)
        promptbytecount += part.text.length;
    }
    const respstream = await gen_ai.models.generateContentStream(modelconfig);
    let candidatesbytecount = 0;
    let mresponse;
    let modelversion;
    let meta;
    for await (mresponse of respstream) {
      //console.log(mresponse); // For debugging only
      for (const candidate of mresponse.candidates) {
        const content = await candidate.content;
         // FIXME There is lots of more information here
        //  GroundingUrls etc.
	for (const part of content.parts)
          candidatesbytecount += part.text.length;
      }
      const content = mresponse.candidates[0]?.content;
      meta = mresponse.usageMetadata;
      modelversion = mresponse.modelVersion;
      contents.push(content);
      const fullResponse = content.parts;
      if (meta.promptTokenCount) {
        if (isVerboseMode)
          console.info("\nUsage Metadata:", meta);
        if (isJsonMode) {
	  meta = {
	    modelVersion: mresponse.modelVersion,
	    promptTokenCount: meta.promptTokenCount,
	    candidatesTokenCount: meta.candidatesTokenCount,
	    promptBytesPerToken:
	     parseFloat((promptbytecount/meta.promptTokenCount).toFixed(1)),
	    candidatesBytesPerToken:
	     parseFloat((candidatesbytecount/meta.candidatesTokenCount)
	                .toFixed(1)),
	  };
          fullResponse.push({"usageMetadata": JSON.stringify(meta, null, 2)});
	}
      }
      yield fullResponse;
    }
  } catch (error) {
    if (error.name === "AbortError") {
        // Supply "final" data
      yield ([{"usageMetadata": JSON.stringify({"abort": 1}, null, 2)}]);
      //   Silently ignore this error, since it is user requested
      return;
    }
    console.error("Error calling Vertex AI Gemini API:", error);
    if (error.details)
      console.error("Error details:", error.details);
    throw error;
  }
}

// Global chat history.
let chatHistory = [];

async function sendMessageInteractive(message) {
  chatHistory.push({ "role": "user", "parts": [{ "text": message }] });

  try {
     // Call the API with the full current chat history.
    //  Let the model implicitly handle caching of common prefixes.
    for await (const modelResponse of
               sendtoLLM({systemInstruction: cmdline_def_systeminstruction},
		chatHistory)) {
      for (const part of modelResponse) {
        if (part.text != null)
          process.stdout.write(part.text);
      }
    }
  } catch (error) {
     // If an error occurs, remove the last user message
    //  to avoid a broken turn in history.
    chatHistory.pop();
  }
}

async function startChat() {   // Read all data from stdin
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
    terminal: !isJsonMode, // Only use terminal features (like prompt)
                          //  in interactive mode
  });

  if (process.argv.includes("-h")) {
    showusage(`Usage: regurge [-j] [-v] [-P project] [-L location] [-M model]
\t-j\tJSON input
\t-P\tVertexAI-project (environment VERTEXAI_PROJECT)
\t-L\tVertexAI-location (environment VERTEXAI_LOCATION)
\t-M\tVertexAI-model (default: ` + modelname + `)
\t-v\tVerbose LLM output`);
  }

  var systemconfig = {}

  if (isJsonMode) {
      // JSON Mode: Expect JSON objects on stdin.  Arrays replace history.
     //  Objects append to history.
    let jsonChatHistory = [];
    let systemconfig = [];
    let previouslines = "";

     // FIXME This line based parsing breaks down if multiple separate
    //  JSON objects are given on a single line of input.
    rl.on("line", async (line) => { // Process each line as a potential
      try {                        //  JSON object
        const parsedInput = JSON.parse(previouslines + line);
	previouslines = "";	 //    Clear parsing history

         // If input is an array, assume it's the initial full
	//  conversation history.
        if (Array.isArray(parsedInput)) {
	  // Convert an initial model role into system instructions
	  if (parsedInput.length && parsedInput[0].role === "model") {
	    let accu = "";
	    for (const part of parsedInput.shift().parts)
	      accu += part.text || "";
	    try {
	      systemconfig = eval("({" + accu + "})");
	      if (!Array.isArray(systemconfig.systemInstruction))
	        systemconfig.systemInstruction
		 = [systemconfig.systemInstruction];
	      const parts = [];
	      for (const part of systemconfig.systemInstruction)
	        parts.push({text: part});
	      systemconfig.systemInstruction = parts;
	    } catch (error) {
	      console.error(error);
	    }
	  }
          jsonChatHistory = parsedInput;
	} else if (parsedInput && typeof parsedInput === "object") {
	  if (parsedInput.abort) {
	      // Signal abort to the running stream
	    abortctrl?.abort();
	    //   Ignore the rest of the message
	    return;
	  }
          // Otherwise, assume it's a single message object to append.
          jsonChatHistory.push(parsedInput);
        } else {
          console.error(
            "Error: Invalid JSON input. Expected an object or an array " +
            "for initial history."
          );
          return; // Skip processing this line
        }

         // Send the entire accumulated history to the LLM
        //  Let the model implicitly handle caching of common prefixes.
	try {
	  for await (const fullResponse of
	             sendtoLLM(systemconfig, jsonChatHistory)) {
	    // Just send back the model's response
            process.stdout.write(JSON.stringify(fullResponse) + "\n");
	  }
	} catch (error) {
          console.error("Connection to LLM died.");
          process.exit(2);
	}
      } catch (error) {
	previouslines += line;
        // Continue waiting for more input even if one line fails
      }
    });

    rl.on("close", () => { // stdin closed, exit
      console.error("JSON input stream ended. Exiting.");
      process.exit(0);
    });
  } else {
    // Interactive Mode: Use readline for continuous input with prompt.
    rl.prompt();                     // Show initial prompt
    rl.on("line", async (line) => { //  Process each line as user input
      await sendMessageInteractive(line.trim());
      rl.prompt();                //    Show prompt again for next input
    }).on("close", () => {       //     stdin closed, exit
      console.error("Interactive chat ended. Exiting.");
      process.exit(0);
    });
  }
}

// Start the chat application.
startChat();
